{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding Gradient \n",
    "\n",
    "## S.Alireza Mousavizade\n",
    "\n",
    "***\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Artificial Neural Networks as we know were invented in 1943 to mimic our biological Nervous system to help machines learn as humans do.\n",
    "But it was not until 1975 that we were able to actually make machines learn and recognize patterns in a data, with the famous Back-Propagation Algorithm came a new hope of training of multi-layered networks.\n",
    "It allowed researchers to train supervised deep artificial neural networks from scratch, although with a little success. The Problem for this low accuracy of training the ANN using Back Propagation was later identified by Sepp Hochreiter’s in 1991.\n",
    "\n",
    "# The Problem\n",
    "1. The Vanishing Gradients\n",
    "2. The Exploding Gradients\n",
    "\n",
    "## The Vanishing Gradients\n",
    "\n",
    "In Deep Neural Networks **adding more and more hidden layers** makes our network to learn **more Complex arbitrary functions** and features and therefore have higher accuracy while predicting the outcomes or identifying a pattern/feature in a complex data such as Image and Speech.\n",
    "\n",
    "But, **adding a layer comes at a cost** which we refer as the **Vanishing Gradient**.\n",
    "The Error that is back propagated using the Back Propagation Algorithm might become so small by the time it reaches the input of the model that it may have very little effect. This phenomena is called the Vanishing Gradient Problem\n",
    "This make it difficult to know which direction the parameters/weights should move to improve the cost function therefore causes premature conversion to a poor solution.\n",
    "\n",
    "below is an example of how mathematically back propagation works for a 4 hidden layer network.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/vanishing_and_exploding_gradients.jpg\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "Some time it might so happen that ∂J/∂b1 becomes equal to zero, and hence may not contribute towards updation of weights, thus causing a premature end to the learning of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exploding Gradients\n",
    "\n",
    "Let’s now talk about another scenario that is very common with Deep Neural Nets that leads to failure of model training.\n",
    "\n",
    "Sometimes it might so happen that while updating the weights error gradients can accumulate and result in Large gradients, this is in turn result in large update of weights and therefore make a network unstable, worst case scenario being that the value of weights become NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions to the Vanishing Gradient Problem\n",
    "\n",
    "## 1. Activation Function\n",
    "\n",
    "The Simplest solution is to use activation functions like relu (leaky relu instead of sigmoid, tanh. \n",
    "[The Vanishing Gradient Problem Of Sigmoid](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "\n",
    "### Sigmoid:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/sigmoid_vae_gradients.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "### Tanh:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/tanh_vae_gradients.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight Initialization\n",
    "\n",
    "- How to choose the\tstarting point for the iterative process of optimization?\n",
    "\n",
    "- The aim of weight initialization is prevent layer activation outputs\tfrom exploding or vanishing\tduring the course of a forward pass.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/weight_initialization.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "## Ideas:\n",
    "\n",
    "### Small Random Numbers\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi1.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "### Xavier Initialization\n",
    "\n",
    "#### Steps to Derieve:\n",
    "\n",
    "#### Step 1:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi2.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 2:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi3.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 3:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi4.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 4:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi5.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 5:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi6.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 6:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi7.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 7:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi8.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Connections\n",
    "\n",
    "the residual connection directly adds the value present at the beginning of the block, x, to the end of the block (F(x)+x) thus residual connection doesn’t have to go through the  activation functions that “squashes” the derivatives, resulting in a higher overall derivative of the block.\n",
    "\n",
    "### ResNet Architecture\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/resnet_arch.jpeg\" alt=\"Drawing\" style=\"width: 95%;\"/>\n",
    "</center>\n",
    "\n",
    "> **Bottleneck Design:** The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the ResNet architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.\n",
    "\n",
    "| Residual Block Connection | Bottleneck Design | \n",
    "| -- | -- | \n",
    "| ![](vae_gradients_images/residual_block_connection.png) | ![Residual Network](vae_gradients_images/resnet_bottleneck.png) |\n",
    "\n",
    "### ResNet Performance\n",
    "\n",
    "Winner results of the ImageNet large scale visual recognition challenge (LSVRC) of the past years on the top-5 classiﬁcation task.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/image_net_results.jpg\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "### [Current State-of-the-art Models](https://paperswithcode.com/sota/image-classification-on-imagenet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GRU and LSTM For RNN Cases\n",
    "\n",
    "These two topics will be explained later.\n",
    "\n",
    "## 5. Batch Normalization Layers\n",
    "\n",
    "Batch Normalization (BN) does not prevent the vanishing or exploding gradient problem in a sense that these are impossible. Rather it reduces the probability for these to occur. Accordingly, the original paper states: ( It was proposed by Sergey Ioffe and Christian Szegedy in 2015.)\n",
    "\n",
    "> In traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normalization helps address these issues. By normalizing activations throughout the network, it prevents small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients; for instance, it prevents the training from getting stuck in the saturated regimes of nonlinearities.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the Vanishing Gradient Problem\n",
    "\n",
    "1. **Gradient Clipping**: When gradients explode, the gradients could become NaN because of the numerical overflow or we might see irregular oscillations in training cost when we plot the learning curve. A solution to fix this is to apply Gradient Clipping; which places a predefined threshold on the gradients to prevent it from getting too large, and by doing this it doesn’t change the direction of the gradients it only change its length.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/gradient-clipping.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "2. **Network Re-designing**: Using a smaller batch size and fewer layers while training might show some improvement in tackling the Exploding Gradients.\n",
    "\n",
    "3. **LSTM Networks**: Using LSTMs and perhaps related gated-type neuron structures are the new best practices to avoid exploding gradients in networks.\n",
    "\n",
    "4. **Weight Regularization**: if exploding gradients are still occurring, is to check the size of network weights and apply a penalty to the networks loss function for large weight values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook vae_gradients.ipynb to html\n",
      "[NbConvertApp] Writing 585373 bytes to vae_gradients.html\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!jupyter nbconvert --to html vae_gradients.ipynb"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0ebfc267f4a7b44e6360165375d376b7715406d7f7a992e90ad755341a1997f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
