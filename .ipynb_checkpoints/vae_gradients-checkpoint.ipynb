{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing and Exploding Gradient \n",
    "\n",
    "Artificial Neural Networks as we know were invented in 1943 to mimic our biological Nervous system to help machines learn as humans do.\n",
    "But it was not until 1975 that we were able to actually make machines learn and recognize patterns in a data, with the famous Back-Propagation Algorithm came a new hope of training of multi-layered networks.\n",
    "It allowed researchers to train supervised deep artificial neural networks from scratch, although with a little success. The Problem for this low accuracy of training the ANN using Back Propagation was later identified by Sepp Hochreiter’s in 1991.\n",
    "\n",
    "### The Problem\n",
    "1. The Vanishing Gradients\n",
    "2. The Exploding Gradients\n",
    "\n",
    "### The Vanishing Gradients\n",
    "\n",
    "In Deep Neural Networks **adding more and more hidden layers** makes our network to learn **more Complex arbitrary functions** and features and therefore have higher accuracy while predicting the outcomes or identifying a pattern/feature in a complex data such as Image and Speech.\n",
    "\n",
    "But, **adding a layer comes at a cost** which we refer as the **Vanishing Gradient**.\n",
    "The Error that is back propagated using the Back Propagation Algorithm might become so small by the time it reaches the input of the model that it may have very little effect. This phenomena is called the Vanishing Gradient Problem\n",
    "This make it difficult to know which direction the parameters/weights should move to improve the cost function therefore causes premature conversion to a poor solution.\n",
    "\n",
    "below is an example of how mathematically back propagation works for a 4 hidden layer network.\n",
    "\n",
    "![Vanishing Gradient](vae_gradients_images/vanishing_and_exploding_gradients.jpg)\n",
    "\n",
    "Some time it might so happen that ∂J/∂b1 becomes equal to zero, and hence may not contribute towards updation of weights, thus causing a premature end to the learning of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Exploding Gradients\n",
    "\n",
    "Let’s now talk about another scenario that is very common with Deep Neural Nets that leads to failure of model training.\n",
    "\n",
    "Sometimes it might so happen that while updating the weights error gradients can accumulate and result in Large gradients, this is in turn result in large update of weights and therefore make a network unstable, worst case scenario being that the value of weights become NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solution to the Vanishing Gradient Problem\n",
    "\n",
    "## 1. Activation Function\n",
    "\n",
    "The Simplest solution is to use activation functions like relu (leaky relu instead of sigmoid, tanh. \n",
    "[The Vanishing Gradient Problem Of Sigmoid](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "\n",
    "### Sigmoid:\n",
    "\n",
    "![Sigmoid](vae_gradients_images/sigmoid_vae_gradients.png)\n",
    "\n",
    "### Tanh:\n",
    "\n",
    "![Tanh](vae_gradients_images/tanh_vae_gradients.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Weight Initialization\n",
    "\n",
    "- #### How to choose the\tstarting point for the iterative process of optimization?\n",
    "\n",
    "- #### The aim of weight initialization is prevent layer activation outputs\tfrom exploding or vanishing\tduring the course of a forward pass.\n",
    "\n",
    "![Weight Init](vae_gradients_images/weight_initialization.png)\n",
    "\n",
    "## Ideas:\n",
    "\n",
    "### Small Random Numbers:\n",
    "\n",
    "![](vae_gradients_images/wi1.png)\n",
    "\n",
    "### Xavier Initialization\n",
    "\n",
    "### 1\n",
    "![](vae_gradients_images/wi2.png)\n",
    "\n",
    "### 2\n",
    "![](vae_gradients_images/wi3.png)\n",
    "\n",
    "### 3\n",
    "![](vae_gradients_images/wi4.png)\n",
    "\n",
    "### 4\n",
    "![](vae_gradients_images/wi5.png)\n",
    "\n",
    "### 5\n",
    "![](vae_gradients_images/wi6.png)\n",
    "\n",
    "### 6\n",
    "![](vae_gradients_images/wi7.png)\n",
    "\n",
    "### 7\n",
    "![](vae_gradients_images/wi8.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Residual Connections\n",
    "\n",
    "the residual connection directly adds the value present at the beginning of the block, x, to the end of the block (F(x)+x) thus residual connection doesn’t have to go through the  activation functions that “squashes” the derivatives, resulting in a higher overall derivative of the block.\n",
    "\n",
    "![Residual Network](vae_gradients_images/residual_block.png)\n",
    "\n",
    "### ResNet Architecture\n",
    "\n",
    "![Resnet](vae_gradients_images/resnet_arch.jpeg)\n",
    "\n",
    "> **Bottleneck Design:** The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the ResNet architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.\n",
    "\n",
    "![](vae_gradients_images/bottleneck.png)\n",
    "\n",
    "\n",
    "\n",
    "### ResNet Performance\n",
    "\n",
    "Winner results of the ImageNet large scale visual recognition challenge (LSVRC) of the past years on the top-5 classiﬁcation task.\n",
    "\n",
    "![Performance](vae_gradients_images/image_net_results.jpg)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a0ebfc267f4a7b44e6360165375d376b7715406d7f7a992e90ad755341a1997f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 ('MyEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
