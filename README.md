# Presentation For Neural Networks Seminar (Recurrent Neural Networks)

## The topics covered in this seminar:

1. Vanishing Gradients & Solutions
   - Activation Functions (Sigmoid, Tanh, ReLu, Leaky ReLu)
   - Weight Initialization (Xavier Initialization)
   - Residual Block Connections (ResNet Architecture)
   - Gated RNNs (GRU, LSTM, ...)
   - Batch Normalization

2. Exploding Gradients & Solutions
   - Gradient Clipping
   - Network Re-designing
   - LSTM Networks
   - Weight Regularization

3. Recurrent Neural Networks 
   - Motivation
   - Architecture
   - Advantages & Drawbacks
   - Applications
   - Text Processing & Word Embedding + Code
   - Gated RNNs 
     - Motivation
     - GRU & LSTM Architecture 
     - Compare SimpleRNN, LSTM & GRU + Code
   - Variants of RNNs
     - Bidirectional RNNs
     - Deep RNNs 
   - CNN & RNN Combination
     - 1D Convolutional Layer
   - C-LSTM
   - Some RNNs Applications Architecture
  