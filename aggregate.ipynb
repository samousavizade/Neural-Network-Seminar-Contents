{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Presentation For Neural Networks Seminar (Recurrent Neural Networks)\n",
    "\n",
    "## S.Alireza Mousavizade\n",
    "\n",
    "## The topics covered in this presentation:\n",
    "\n",
    "1. Vanishing Gradients & Solutions\n",
    "   - Activation Functions (Sigmoid, Tanh, ReLu, Leaky ReLu)\n",
    "   - Weight Initialization (Xavier Initialization)\n",
    "   - Residual Block Connections (ResNet Architecture)\n",
    "   - Gated RNNs (GRU, LSTM, ...)\n",
    "   - Batch Normalization\n",
    "\n",
    "2. Exploding Gradients & Solutions\n",
    "   - Gradient Clipping\n",
    "   - Network Re-designing\n",
    "   - LSTM Networks\n",
    "   - Weight Regularization\n",
    "\n",
    "3. Recurrent Neural Networks\n",
    "   - Motivation\n",
    "   - Architecture\n",
    "   - Advantages & Drawbacks\n",
    "   - Applications\n",
    "   - Text Processing & Word Embedding + Code\n",
    "   - Gated RNNs\n",
    "     - Motivation\n",
    "     - GRU & LSTM Architecture\n",
    "     - Compare SimpleRNN, LSTM & GRU + Code\n",
    "   - Variants of RNNs\n",
    "     - Bidirectional RNNs\n",
    "     - Deep RNNs\n",
    "   - CNN & RNN Combination\n",
    "     - 1D Convolutional Layer\n",
    "   - C-LSTM\n",
    "   - Some RNNs Applications Architecture\n",
    "\n",
    "***\n",
    "\n",
    "# Vanishing and Exploding Gradient\n",
    "\n",
    "***\n",
    "\n",
    "# Introduction\n",
    "\n",
    "Artificial Neural Networks as we know were invented in 1943 to mimic our biological Nervous system to help machines learn as humans do.\n",
    "But it was not until 1975 that we were able to actually make machines learn and recognize patterns in a data, with the famous Back-Propagation Algorithm came a new hope of training of multi-layered networks.\n",
    "It allowed researchers to train supervised deep artificial neural networks from scratch, although with a little success. The Problem for this low accuracy of training the ANN using Back Propagation was later identified by Sepp Hochreiter’s in 1991.\n",
    "\n",
    "# The Problem\n",
    "1. The Vanishing Gradients\n",
    "2. The Exploding Gradients\n",
    "\n",
    "## The Vanishing Gradients\n",
    "\n",
    "In Deep Neural Networks **adding more and more hidden layers** makes our network to learn **more Complex arbitrary functions** and features and therefore have higher accuracy while predicting the outcomes or identifying a pattern/feature in a complex data such as Image and Speech.\n",
    "\n",
    "But, **adding a layer comes at a cost** which we refer as the **Vanishing Gradient**.\n",
    "The Error that is back propagated using the Back Propagation Algorithm might become so small by the time it reaches the input of the model that it may have very little effect. This phenomena is called the Vanishing Gradient Problem\n",
    "This make it difficult to know which direction the parameters/weights should move to improve the cost function therefore causes premature conversion to a poor solution.\n",
    "\n",
    "below is an example of how mathematically back propagation works for a 4 hidden layer network.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/vanishing_and_exploding_gradients.jpg\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "Some time it might so happen that ∂J/∂b1 becomes equal to zero, and hence may not contribute towards updation of weights, thus causing a premature end to the learning of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### The Exploding Gradients\n",
    "\n",
    "Let’s now talk about another scenario that is very common with Deep Neural Nets that leads to failure of model training.\n",
    "\n",
    "Sometimes it might so happen that while updating the weights error gradients can accumulate and result in Large gradients, this is in turn result in large update of weights and therefore make a network unstable, worst case scenario being that the value of weights become NaN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Solutions to the Vanishing Gradient Problem\n",
    "\n",
    "## 1. Activation Function\n",
    "\n",
    "The Simplest solution is to use activation functions like relu (leaky relu instead of sigmoid, tanh.\n",
    "[The Vanishing Gradient Problem Of Sigmoid](https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484)\n",
    "\n",
    "### Sigmoid:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/sigmoid_vae_gradients.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "### Tanh:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/tanh_vae_gradients.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Weight Initialization\n",
    "\n",
    "- How to choose the\tstarting point for the iterative process of optimization?\n",
    "\n",
    "- The aim of weight initialization is prevent layer activation outputs\tfrom exploding or vanishing\tduring the course of a forward pass.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/weight_initialization.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "## Ideas:\n",
    "\n",
    "### Small Random Numbers\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi1.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "### Xavier Initialization\n",
    "\n",
    "#### Steps to Derieve:\n",
    "\n",
    "#### Step 1:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi2.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 2:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi3.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 3:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi4.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 4:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi5.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 5:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi6.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 6:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi7.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n",
    "***\n",
    "\n",
    "#### Step 7:\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/wi8.png\" alt=\"Drawing\" style=\"width: 85%;\"/>\n",
    "</center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Residual Connections\n",
    "\n",
    "the residual connection directly adds the value present at the beginning of the block, x, to the end of the block (F(x)+x) thus residual connection doesn’t have to go through the  activation functions that “squashes” the derivatives, resulting in a higher overall derivative of the block.\n",
    "\n",
    "### ResNet Architecture\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/resnet_arch.jpeg\" alt=\"Drawing\" style=\"width: 95%;\"/>\n",
    "</center>\n",
    "\n",
    "> **Bottleneck Design:** The use of a bottleneck reduces the number of parameters and matrix multiplications. The idea is to make residual blocks as thin as possible to increase depth and have less parameters. They were introduced as part of the ResNet architecture, and are used as part of deeper ResNets such as ResNet-50 and ResNet-101.\n",
    "\n",
    "| Residual Block Connection | Bottleneck Design |\n",
    "| -- | -- |\n",
    "| ![](vae_gradients_images/residual_block_connection.png) | ![Residual Network](vae_gradients_images/resnet_bottleneck.png) |\n",
    "\n",
    "### ResNet Performance\n",
    "\n",
    "Winner results of the ImageNet large scale visual recognition challenge (LSVRC) of the past years on the top-5 classiﬁcation task.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/image_net_results.jpg\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "### [Current State-of-the-art Models](https://paperswithcode.com/sota/image-classification-on-imagenet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. GRU and LSTM For RNN Cases\n",
    "\n",
    "These two topics will be explained later.\n",
    "\n",
    "## 5. Batch Normalization Layers\n",
    "\n",
    "Batch Normalization (BN) does not prevent the vanishing or exploding gradient problem in a sense that these are impossible. Rather it reduces the probability for these to occur. Accordingly, the original paper states: ( It was proposed by Sergey Ioffe and Christian Szegedy in 2015.)\n",
    "\n",
    "> In traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normalization helps address these issues. By normalizing activations throughout the network, it prevents small changes to the parameters from amplifying into larger and suboptimal changes in activations in gradients; for instance, it prevents the training from getting stuck in the saturated regimes of nonlinearities.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Solution to the Vanishing Gradient Problem\n",
    "\n",
    "1. **Gradient Clipping**: When gradients explode, the gradients could become NaN because of the numerical overflow or we might see irregular oscillations in training cost when we plot the learning curve. A solution to fix this is to apply Gradient Clipping; which places a predefined threshold on the gradients to prevent it from getting too large, and by doing this it doesn’t change the direction of the gradients it only change its length.\n",
    "\n",
    "<center>\n",
    "<img src=\"vae_gradients_images/gradient-clipping.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "2. **Network Re-designing**: Using a smaller batch size and fewer layers while training might show some improvement in tackling the Exploding Gradients.\n",
    "\n",
    "3. **LSTM Networks**: Using LSTMs and perhaps related gated-type neuron structures are the new best practices to avoid exploding gradients in networks.\n",
    "\n",
    "4. **Weight Regularization**: if exploding gradients are still occurring, is to check the size of network weights and apply a penalty to the networks loss function for large weight values.\n",
    "\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "## S.Alireza Mousavizade\n",
    "\n",
    "***\n",
    "\n",
    "# Motivation\n",
    "\n",
    "- Not all problems can be converted into one with fixed-lenght inputs and outputs (such as: texts, signal, time series and etc.)\n",
    "- Problems such as speech recognition or time-series prediction require a system to store and usethe context information\n",
    "  - Simple case: Output YES if the number of 1s id odd, else NO. (There is no constraint on the length of the input sequence.)\n",
    "\n",
    " - In sequential data, the order of the data must also be considered. (For example: in a time series the price of a stock)\n",
    "\n",
    "***\n",
    "\n",
    "# Architecture\n",
    "\n",
    "Recurrent neural networks, also known as RNNs, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states. They are typically as follows:\n",
    "\n",
    "Input: $x^{<1>}, x^{<2>}, ..., x^{<t>}$ where $x^{<i>}$ can be a number, a vector, a matrix, or even a tensor.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/rnn_archit.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "- An RNN shares the same weights and bias parameters across several time steps\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "# The pros and cons\n",
    "\n",
    "The pros and cons of a typical RNN architecture are summed up in the table below:\n",
    "\n",
    "| Advantages | Drawbacks |\n",
    "| -- | -- |\n",
    "| Possibility of processing input of any length | Computation being slow |\n",
    "| Model size not increasing with size of input | Difficulty of accessing information from a long time ago |\n",
    "| Computation takes into account historical information | Cannot consider any future input for the current state\n",
    "| Weights are shared across time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "# Applications of RNNs\n",
    "\n",
    "RNN models are mostly used in the fields of natural language processing and speech recognition. The different applications are summed up in the table below:\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/rnn_applications.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "## One To Many\n",
    "\n",
    "One-to-many sequence problems are sequence problems where the input data has one time-step, and the output contains a vector of multiple values or multiple time-steps. Thus, we have a single input and a sequence of outputs.\n",
    "\n",
    "### Music Generation\n",
    "\n",
    "**OpenAI** created [MuseNet](https://openai.com/blog/musenet/), a deep neural network that can generate 4-minute musical compositions with 10 different instruments, and can combine styles from country to Mozart to the Beatles. MuseNet was not explicitly programmed with our understanding of music, but instead discovered patterns of harmony, rhythm, and style by learning to predict the next token in hundreds of thousands of MIDI files. MuseNet uses the same general-purpose unsupervised technology as GPT-2, a large-scale transformer model trained to predict the next token in a sequence, whether audio or text.\n",
    "\n",
    "### Image Captioning\n",
    "\n",
    "Image Captioning is the task of describing the content of an image in words Check out this amazing [\"Generate Meaningful Captions for Images with Attention Models\"](https://wandb.ai/authors/image-captioning/reports/Generate-Meaningful-Captions-for-Images-with-Attention-Models--VmlldzoxNzg0ODA) report by Rajesh Shreedhar Bhat and Souradip Chakraborty to learn more.\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/image_captioning.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "## Many To One\n",
    "\n",
    "In many-to-one sequence problems, we have a sequence of data as input, and we have to predict a single output. Sentiment analysis or text classification is one such use case.\n",
    "\n",
    "### Sentiment Analysis\n",
    "\n",
    "Sentiment analysis (or opinion mining) is a natural language processing (NLP) technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs. **Code**\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/sentiment_analysis.jpeg\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "### Time-Series Forcasting\n",
    "\n",
    "Time series forecasting occurs when you make scientific predictions based on historical time stamped data. It involves building models through historical analysis and using them to make observations and drive future strategic decision-making.\n",
    "\n",
    "[Timeseries forecasting for weather prediction On Climate data time-series](https://keras.io/examples/timeseries/timeseries_weather_forecasting/)\n",
    "\n",
    "## Many To Many\n",
    "\n",
    "Many-to-Many sequence learning can be used for machine translation where the input sequence is in some language, and the output sequence is in some other language. It can be used for Video Classification as well, where the input sequence is the feature representation of each frame of the video at different time steps.\n",
    "\n",
    "Encoder-Decoder network is commonly used for many-to-many sequence tasks. Here encoder-decoder is just a fancy name for a neural architecture with two LSTM layers.\n",
    "\n",
    "### Tx $=$ Ty:\n",
    "\n",
    "#### Named Entity Recognition\n",
    "\n",
    "A named entity is a “real-world object” that’s assigned a name – for example, a person, a country, a product or a book title. spaCy can recognize various types of named entities in a document, by asking the model for a prediction. Because models are statistical and strongly depend on the examples they were trained on, this doesn’t always work perfectly and might need some tuning later, depending on your use case.\n",
    "[spaCy library code](https://spacy.io/usage/linguistic-features/)\n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/name_entity.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "### Tx $\\neq$ Ty:\n",
    "\n",
    "- Many to One + One to Many\n",
    "  - Many to One: Encode input sequence in a single vector.\n",
    "  - One to Many: Decode output sequence from single input vector.\n",
    "\n",
    "#### Machine Translation\n",
    "\n",
    "Machine translation, sometimes referred to by the abbreviation MR (not to be confused with computer-aided translation, machine-aided human translation or interactive translation), is a sub-field of computational linguistics that investigates the use of software to translate text or speech from one language to another.\n",
    "\n",
    "- Why reverse input sequence?\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/sequence_to_sequence.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "#### Video Object Detection [Link](https://kth.diva-portal.org/smash/get/diva2:1156631/FULLTEXT01.pdf)\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/video_object_detection.png\" alt=\"Drawing\" style=\"width: 60%;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "# Sequential Data: Text\n",
    "\n",
    "- Text can be understood as either a sequence of characters or a sequence of words.\n",
    "- Deep Learning for natural-language processing is pattern recognition applied to words, sentences, and paragraphs. in much the same way that deep learning for computer vision is pattern recognition applied to pixels.\n",
    "\n",
    "- Applications include document classification, sentiment analysis, author identification, and even question-answering (QA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "## Text Processing\n",
    "\n",
    "Like all other neural networks, deep learning models dont take as input raw text: the only work with numeric tensors.\n",
    "Vectoring text:\n",
    "  - Segment text into words, and transform each word a vector.\n",
    "  - Segment text into characters, and transform each character into a vector.\n",
    "  - Extract n-grams of words or characters and transform each n-gram into a vector.\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "The different units into which you can break down text(words, characters, or n-grams) are called tokens, and breaking text into such tokens is called tokenization.\n",
    "\n",
    "- All text-vectorization processes consist of applying some tokenization scheme and then associating numeric vectors with the generated tokens.\n",
    "\n",
    "## Word Embeddings\n",
    "\n",
    "A word embedding is a learned representation for text where words that have the same meaning have a similar representation. It is this approach to representing words and documents that may be considered one of the key breakthroughs of deep learning on challenging natural language processing problems\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/word_embedding.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "- **Word Embeddings** are pack more information into far fewer dimensions rather than **one-hot encoding**.\n",
    "\n",
    "- They can be pre-trained on large amounts of text training data.\n",
    "\n",
    "There are two ways to obtain word embeddings:\n",
    "\n",
    "- Learn word embeddings jointly with the main mask\n",
    "  - Start with random word vectors.\n",
    "- Load word embeddings that were pre-trained using different machine learning task. [Distributed Representations of Words and Phrases and their Compositionality](https://arxiv.org/abs/1310.4546)\n",
    "  - Called pre-trained word embeddings\n",
    "\n",
    "## Learning Word Embeddings\n",
    "\n",
    "- Associate a random vector to each word\n",
    "- The problem with this approach is that the resulting embedding space has no structure\n",
    "   - For instance, the words accurate and exact may end up with completely different embeddings even though they are interchangeable in most sentences.\n",
    " - The geometric relationships between word vectors should reflect the semantic relationships between these words.\n",
    " - Word embeddings are meant to map human language into a geometric space.\n",
    " - In a reasonable embedding space, you would expect synonyms to be embedded into similar word vectors.\n",
    "- We expect the geometric distance between any ttwo word vectors to relate to he semantic distance between the associated words.\n",
    "- We may want specific directions in the embedding space to be meaningful.\n",
    "\n",
    "- A good word embedding space depends heaviy on your task.\n",
    "- Reasonable to learn a new embedding space with every new task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Embedding Layer (Keras)\n",
    "\n",
    "Embedding layer is a dictionary that maps integer indices (which stand for specific words) to dence vectors. Ductionary like 2D array of shape (total_number_of_words, embedding_dimensionality)\n",
    "\n",
    "- Takes as input a 2D tesnor of integers of shape (n_samples, seqeunce_length) where n_sample is batch size.\n",
    "- (32, 10): batch of 32 sequences of length 10. (**length is fixed.**)\n",
    "- Returns a 3D floating-point tensor of shape (n_samples, seqeunce_length, embedding_dimensionality)\n",
    "  -  ```\n",
    "     from keras.layers import Embedding\n",
    "     embedding = Embedding(input_dim=1000, output_dim=64)\n",
    "     ```\n",
    "\n",
    "- When we instantiate an Embedding layer, its weights (its internal dictionary of token vectors) are initially random, just as with any other layer.\n",
    "- During training, these word vectors are gradually adjusted via backpropagation, structuring the space into something the downstream model can exploit.\n",
    "\n",
    "## IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 10:55:20.353757: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:20.353773: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "# Disable warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "\n",
    "max_features = 10000\n",
    "maxlen = 40\n",
    "embedding_dimension = 8\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
    "\n",
    "x_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 40, 8)             80000     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 320)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 321       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 80,321\n",
      "Trainable params: 80,321\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 10:55:26.085612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-04-10 10:55:26.085819: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.085865: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublas.so.11'; dlerror: libcublas.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.085907: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcublasLt.so.11'; dlerror: libcublasLt.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.085948: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcufft.so.10'; dlerror: libcufft.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.085989: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcurand.so.10'; dlerror: libcurand.so.10: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.086030: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.086070: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusparse.so.11'; dlerror: libcusparse.so.11: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.086111: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudnn.so.8'; dlerror: libcudnn.so.8: cannot open shared object file: No such file or directory\n",
      "2022-04-10 10:55:26.086120: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2022-04-10 10:55:26.086369: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "625/625 [==============================] - 1s 1ms/step - loss: 0.6602 - acc: 0.6364 - val_loss: 0.5775 - val_acc: 0.7432\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 1s 991us/step - loss: 0.4804 - acc: 0.7896 - val_loss: 0.4587 - val_acc: 0.7794\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3906 - acc: 0.8276 - val_loss: 0.4301 - val_acc: 0.7958\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3505 - acc: 0.8461 - val_loss: 0.4247 - val_acc: 0.8016\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3247 - acc: 0.8612 - val_loss: 0.4246 - val_acc: 0.8020\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.3048 - acc: 0.8711 - val_loss: 0.4283 - val_acc: 0.8022\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 1s 998us/step - loss: 0.2866 - acc: 0.8801 - val_loss: 0.4324 - val_acc: 0.8016\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 1s 1ms/step - loss: 0.2692 - acc: 0.8877 - val_loss: 0.4403 - val_acc: 0.7984\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 1s 961us/step - loss: 0.2519 - acc: 0.8971 - val_loss: 0.4457 - val_acc: 0.7960\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 1s 974us/step - loss: 0.2345 - acc: 0.9053 - val_loss: 0.4561 - val_acc: 0.7938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff51dec7d60>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(input_dim=max_features, output_dim=embedding_dimension,  input_length=maxlen))\n",
    "\n",
    "# Flatten the 3d tensor of embedding into a 2d tensor of shape (n_samples, maxlen * embedding_dimension)\n",
    "model.add(Flatten())\n",
    "\n",
    "# Class = \"+\" and \"-\" comment\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\", loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          epochs=10,\n",
    "          batch_size=32,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***\n",
    "\n",
    "# Gated RNN\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Recurrent Neural Networks (RNN) are good at processing sequence data for predictions. Therefore, they are extremely useful for deep learning applications like speech recognition, speech synthesis, natural language understanding, etc.\n",
    "\n",
    "- Recurrent neural networks suffer from **short-term memory**\n",
    "    - RNN's may leave out important information from the beginning.\n",
    "\n",
    "- Example:\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/vae_gradients_rnn.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "\n",
    "## Solution: Gated RNNs\n",
    "\n",
    "Three are three main types of RNNs: SimpleRNN, Long-Short Term Memories ([LSTM](https://www.researchgate.net/publication/13853244_Long_Short-term_Memory)), and Gated Recurrent Units ([GRU](https://arxiv.org/abs/1412.3555)). SimpleRNNs are good for processing sequence data for predictions but suffers from **short-term memory** (**Vanishing Gradient/ Exploding Gradient**). LSTM’s and GRU’s were created as a method to mitigate short-term memory using mechanisms called gates.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "\n",
    "### GRU\n",
    "\n",
    "Gated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but **has fewer parameters** than LSTM, as it lacks an output gate. **GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.**\n",
    "\n",
    "#### Simplified GRU:\n",
    "\n",
    "#### Rules\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\mathbf{\\tilde{c}_t}& = \\mathbf{tanh(W_{ac} \\, a_{t-1} + W_{xc} \\, x_{t} + b_c)}\\\\\n",
    "    \\hline\n",
    "    \\mathbf{c_t}& = \\mathbf{\\Gamma_u \\, \\tilde{c}_t + (1 - \\Gamma_u) \\, c_{t-1}}\\\\\n",
    "    \\mathbf{\\Gamma_u}& = \\mathbf{\\sigma(W_{cu} \\, c_{t-1} + W_{xu} \\, x_t + b_u)}\\\\\n",
    "    \\hline\n",
    "    \\mathbf{a_t}& = \\mathbf{c_t}\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "#### GRU: (+ Relevance Gate)\n",
    "\n",
    "#### Rules\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\mathbf{\\tilde{c}_t}& = \\mathbf{tanh(W_{cc} \\, (\\Gamma_r \\, . \\, c_{t-1}) + W_{xc} \\, x_{t} + b_c)}\\\\\n",
    "    \\hline\n",
    "    \\mathbf{c_t}& = \\mathbf{\\Gamma_u \\, \\tilde{c}_t + (1 - \\Gamma_t) \\, c_{t-1}}\\\\\n",
    "    \\mathbf{\\Gamma_u}& = \\mathbf{\\sigma(W_{cu} \\, c_{t-1} + W_{xu} \\, x_t + b_u)}\\\\\n",
    "    \\mathbf{\\Gamma_r}& = \\mathbf{\\sigma(W_{cr} \\, c_{t-1} + W_{xr} \\, x_t + b_r)}\\\\\n",
    "    \\hline\n",
    "    \\mathbf{a_t}& = \\mathbf{c_t}\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/GRU.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### LSTM\n",
    "\n",
    "Long short-term memory (LSTM) is an artificial recurrent neural network (RNN) architecture used in the field of deep learning (DL). Unlike standard feedforward neural networks, LSTM has feedback connections.\n",
    "\n",
    "#### Rules\n",
    "\n",
    "$$\n",
    "\n",
    "\\begin{aligned}\n",
    "    \\mathbf{\\tilde{c}_t}& = \\mathbf{tanh(W_{ac} \\, a_{t-1} + W_{xc} \\, x_{t} + b_c)}\\\\\n",
    "    \\hline\n",
    "    \\mathbf{c_t}& = \\mathbf{\\Gamma_u \\, \\tilde{c}_t + \\Gamma_f \\, c_{t-1}}\\\\\n",
    "    \\mathbf{\\Gamma_u}& = \\mathbf{\\sigma(W_{cu} \\, c_{t-1} + W_{xu} \\, x_t + b_u)}\\\\\n",
    "    \\mathbf{\\Gamma_f}& = \\mathbf{\\sigma(W_{cf} \\, c_{t-1} + W_{xf} \\, x_t + b_f)}\\\\\n",
    "    \\mathbf{\\Gamma_o}& = \\mathbf{\\sigma(W_{co} \\, c_{t-1} + W_{xo} \\, x_t + b_o)}\\\\\n",
    "    \\hline\n",
    "    \\mathbf{a_t}& = \\Gamma_o \\, . \\, \\mathbf{c_t} \\\\\n",
    "    \\mathbf{\\Gamma_o}& = \\mathbf{\\sigma(W_{co} \\, c_{t-1} + W_{xo} \\, x_t + b_o)}\\\\\n",
    "\\end{aligned}\n",
    "\n",
    "$$\n",
    "\n",
    "[More ...](https://d2l.ai/chapter_recurrent-neural-networks/rnn.html#:~:text=A%20neural%20network%20that%20uses,number%20of%20time%20steps%20increases.)\n",
    "\n",
    "#### Architecture\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/LSTM.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</center>\n",
    "\n",
    "### Used Gates\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/gates.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Weights and Inputs Shape (Batch Size Consideration)\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "    \\textrm{Input} & : \\mathbf{X_t} \\in \\mathbb{R}^{n \\times d}\\\\[10pt]\n",
    "    \\textrm{Weights} & : \\mathbf{W_{x\\{*\\}}} \\in \\mathbb{R}^{d \\times h}\\\\\n",
    "                         & : \\mathbf{W_{a\\{*\\}}} \\in \\mathbb{R}^{d \\times h}\\\\[5pt]\n",
    "\n",
    "                         & : \\mathbf{W_{c\\{*\\}}} \\in \\mathbb{R}^{h \\times h}\\\\[5pt]\n",
    "\n",
    "                         & : \\mathbf{b_{\\{*\\}}} \\in \\mathbb{R}^{1 \\times h}\\\\\n",
    "\n",
    "                         & : \\mathbf{\\Gamma_u \\in \\mathbb{R}^{n \\times h}}\\\\\n",
    "                         & : \\mathbf{\\Gamma_r \\in \\mathbb{R}^{n \\times h}}\\\\\n",
    "                         & : \\mathbf{\\Gamma_f \\in \\mathbb{R}^{n \\times h}}\\\\\n",
    "                         & : \\mathbf{\\Gamma_o \\in \\mathbb{R}^{n \\times h}}\\\\\n",
    "\n",
    "    \\textrm{where:}\\\\\n",
    "\n",
    "    n & : \\textrm{batch-size}\\\\\n",
    "    d & : \\textrm{input dimension (in text: word embedding dimension)}\\\\\n",
    "    h & : \\textrm{number of hidden units}\n",
    "\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## [Sentiment Analysis On Twitter Tweets](https://colab.research.google.com/drive/1V6-kZg3PetL7IJRjy0mLAYdEUzE8DVnM#scrollTo=W65oBdwmkclD) using SimpleRNN, LSTM and GRU\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Variants of RNNs\n",
    "\n",
    "## Bidirectional RNN\n",
    "\n",
    "### Motivation\n",
    "\n",
    "- All of the RNNs we have considered up to now have a “causal” structure.\n",
    "  - The state at time 𝑡 only captures information from the past, $x_1 , … , x_{t-1}$ , and the present input $x_t$\n",
    "- In some applications, we want to output a prediction of $y_t$ which may depend on the whole input sequence.\n",
    "\n",
    "\n",
    "Example:\n",
    "\n",
    "<center>\n",
    "<img src=\"rnn_images/brnn_ex.png\" alt=\"Drawing\" style=\"width: 45%;\"/>\n",
    "</center>\n",
    "\n",
    "### Architecture\n",
    "\n",
    "Bidirectional RNNs combine an RNN that moves\n",
    "forward through time beginning from the start of the\n",
    "sequence with another RNN that moves backward\n",
    "through time beginning from the end of the sequence.\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/brnn_arch.png\" alt=\"Drawing\" style=\"width: 45%;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Deep Recurrent Networks\n",
    "\n",
    "For learning very complex functions sometimes is useful to stack multiple\n",
    "layers of RNNs together to build even deeper versions of these models.\n",
    "\n",
    "### Architecture\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/drnn_arch.png\" alt=\"Drawing\" style=\"width: 45%;\"/>\n",
    "</center>\n",
    "\n",
    "- Unlike D-CNNs for D-RNNs, having three layers is already quite a lot.\n",
    "- D-RNNs have better memory than Single-Layer RNNs because each $x_t$ affects hidden states values in several ways.\n",
    "- The blocks can be SimpleRNN, GRU, or LSTM\n",
    "\n",
    "### [IMDB](https://colab.research.google.com/drive/1Ir3_GSETKbj3-O9T64I9H3ebo1s4i6CO?usp=sharing#scrollTo=1iahs0-or5uh)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# CNN + RNN\n",
    "\n",
    "Such 1D convnets can be competitive with RNNs on certain sequence-\n",
    "processing problems, usually at a considerably cheaper computational cost.\n",
    "\n",
    "- Time can be treated as a spatial dimension, like the height or width of a 2D\n",
    "image\n",
    "\n",
    "## 1D Convolution Layer\n",
    "\n",
    "1D convolution layers can recognize local\n",
    "patterns in a sequence.\n",
    "\n",
    "A pattern learned at a certain position in a\n",
    "sentence can later be recognized at a\n",
    "different position, making 1D convnets\n",
    "translation invariant.\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/1d_convnet.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</center>\n",
    "\n",
    "- We may use stride and 1D pooling.\n",
    "- Because 1D convnets process input patches independently, they aren’t sensitive to the order of the timesteps (beyond a local scale, the size of the convolution windows), unlike RNNs.\n",
    "\n",
    "## Combining RNN with CNN\n",
    "\n",
    "One strategy to combine **the speed and lightness of convolutional layers** with the **order-sensitivity of recurrent layers** is to use a 1D convnet as a preprocessing step before an RNN.\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/cnn+rnn.png\" alt=\"Drawing\" style=\"width: 50%;\"/>\n",
    "</center>\n",
    "\n",
    "- Especially beneficial when **you’re dealing with sequences that are so long they can’t realistically be processed with RNNs**, such as sequences with thousands of steps.\n",
    "\n",
    "- 1D convnets offer a faster alternative to RNNs on some problems, in particular natural language processing tasks.\n",
    "\n",
    "- Because **RNNs are extremely expensive for processing very long sequences, but 1D convnets are cheap**, it can be a good idea to use a 1D convnet as a preprocessing step before an RNN, **shortening the sequence and extracting useful representations for the RNN to process.**\n",
    "\n",
    "## C-LSTM\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/clstm.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "> Stollenga, Marijn Frederik. Advances in humanoid control and perception. Diss. Università della Svizzera italiana, 2016.\n",
    "\n",
    "> F. Xiong, X. Shi, and D. Yeung. \"Spatiotemporal modeling for crowd counting in videos.\" IEEE International Conference on Computer Vision. 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Applications\n",
    "\n",
    "## Image Captioning\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/image_cap_arch.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "> A. Tripathi, S. Srivastava, and R. Kothari. \"Deep Neural Network Based Image Captioning.\" International Conference on Big Data Analytics. Springer, 2018.\n",
    "\n",
    "## Video Analysis (Lip Reading)\n",
    "\n",
    "<center>\n",
    "    <img src=\"rnn_images/lib_read_arch.png\" alt=\"Drawing\" style=\"width: 75%;\"/>\n",
    "</center>\n",
    "\n",
    "> Fernandez-Lopez, Adriana, and Federico M. Sukno. \"Survey on automatic lip-reading in the era of deep learning.\" Image and Vision Computing 78 (2018): 53-72.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nbconvert\n",
      "  Using cached nbconvert-6.4.5-py3-none-any.whl (561 kB)\n",
      "Collecting mistune<2,>=0.8.1\n",
      "  Using cached mistune-0.8.4-py2.py3-none-any.whl (16 kB)\n",
      "Collecting nbformat>=4.4\n",
      "  Using cached nbformat-5.3.0-py3-none-any.whl (73 kB)\n",
      "Collecting testpath\n",
      "  Using cached testpath-0.6.0-py3-none-any.whl (83 kB)\n",
      "Collecting bleach\n",
      "  Using cached bleach-5.0.0-py3-none-any.whl (160 kB)\n",
      "Requirement already satisfied: jupyter-core in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (4.9.2)\n",
      "Collecting defusedxml\n",
      "  Using cached defusedxml-0.7.1-py2.py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (4.11.1)\n",
      "Requirement already satisfied: entrypoints>=0.2.2 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (0.4)\n",
      "Collecting pandocfilters>=1.4.1\n",
      "  Using cached pandocfilters-1.5.0-py2.py3-none-any.whl (8.7 kB)\n",
      "Requirement already satisfied: pygments>=2.4.1 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (2.11.2)\n",
      "Requirement already satisfied: traitlets>=5.0 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (5.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (2.1.1)\n",
      "Collecting jupyterlab-pygments\n",
      "  Using cached jupyterlab_pygments-0.2.0-py2.py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: jinja2>=2.4 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbconvert) (3.1.1)\n",
      "Collecting nbclient<0.6.0,>=0.5.0\n",
      "  Using cached nbclient-0.5.13-py3-none-any.whl (70 kB)\n",
      "Requirement already satisfied: nest-asyncio in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (1.5.5)\n",
      "Requirement already satisfied: jupyter-client>=6.1.5 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from nbclient<0.6.0,>=0.5.0->nbconvert) (7.2.2)\n",
      "Collecting jsonschema>=2.6\n",
      "  Using cached jsonschema-4.4.0-py3-none-any.whl (72 kB)\n",
      "Collecting fastjsonschema\n",
      "  Using cached fastjsonschema-2.15.3-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from beautifulsoup4->nbconvert) (2.3.2)\n",
      "Collecting webencodings\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Requirement already satisfied: six>=1.9.0 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from bleach->nbconvert) (1.16.0)\n",
      "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0\n",
      "  Using cached pyrsistent-0.18.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (119 kB)\n",
      "Collecting importlib-resources>=1.4.0\n",
      "  Using cached importlib_resources-5.6.0-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from jsonschema>=2.6->nbformat>=4.4->nbconvert) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (2.8.2)\n",
      "Requirement already satisfied: pyzmq>=22.3 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (22.3.0)\n",
      "Requirement already satisfied: tornado>=6.0 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from jupyter-client>=6.1.5->nbclient<0.6.0,>=0.5.0->nbconvert) (6.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /home/ahura/anaconda3/envs/DataEnvPIP/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=2.6->nbformat>=4.4->nbconvert) (3.8.0)\n",
      "Installing collected packages: webencodings, mistune, fastjsonschema, testpath, pyrsistent, pandocfilters, jupyterlab-pygments, importlib-resources, defusedxml, bleach, jsonschema, nbformat, nbclient, nbconvert\n",
      "Successfully installed bleach-5.0.0 defusedxml-0.7.1 fastjsonschema-2.15.3 importlib-resources-5.6.0 jsonschema-4.4.0 jupyterlab-pygments-0.2.0 mistune-0.8.4 nbclient-0.5.13 nbconvert-6.4.5 nbformat-5.3.0 pandocfilters-1.5.0 pyrsistent-0.18.1 testpath-0.6.0 webencodings-0.5.1\n",
      "[NbConvertApp] Converting notebook aggregate.ipynb to html\n",
      "[NbConvertApp] Writing 634123 bytes to aggregate.html\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install nbconvert\n",
    "!jupyter nbconvert --to html aggregate.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}